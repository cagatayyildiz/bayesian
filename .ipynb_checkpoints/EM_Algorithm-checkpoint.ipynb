{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary: Jensen's Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\E}[1]{\\left\\langle#1\\right\\rangle}$\n",
    "$\\newcommand{\\par}[1]{\\left(#1\\right)}$\n",
    "\n",
    "If $f$ is a concave function, then for $0\\leq \\lambda \\leq 1$ Jensen's inequality states that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "f(\\lambda x_1 + (1-\\lambda)x_2) \\geq \\lambda f(x_1) + (1-\\lambda)f(x_2)\n",
    "\\end{eqnarray}\n",
    "In general, for constants $c_1, c_2, c_3, ...$ that sum up to 1, we can generalize the above statement as \n",
    "\n",
    "\\begin{eqnarray}\n",
    "f\\left(\\sum_i c_i x_i\\right) \\geq \\sum_i c_i f(x_i)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Whenever $p(x_i) = c_i$, the statement is written as \n",
    "\\begin{eqnarray}\n",
    "f(\\E{x}) \\geq \\E{f(x)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Now, let's see $\\log$ is a concave function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa08583c0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.arange(1e-3,1e3,0.1)\n",
    "Y = np.log(X)\n",
    "\n",
    "plt.plot(X,Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[see http://www.cs.princeton.edu/courses/archive/spr08/cos424/scribe_notes/0311.pdf for better writing]\n",
    "\n",
    "[see the EM Algorithm section (**11.4**) in Kevin Murphy's book]\n",
    "\n",
    "[also see Barış's website for Taylan Hoca's notation: http://bariskurt.com/learning-markov-mixtures-with-em-derivationmatlab-code/]\n",
    "\n",
    "\n",
    "The EM algorithm is a general method that is widely used to find the parameters of a state space model that maximizes the likelihood. If $\\theta$, $z$ and $x$ denote model parameters, latent variables and observations, respectively, our goal is to maximize log-likelihood $\\mathcal{L}(\\theta)$:\n",
    "\n",
    "\\begin{align}\n",
    "   \\hat{\\theta} &= \\underset{\\theta}{\\text{argmax}}\\mathcal{L}(\\theta)\\\\\n",
    "   &=\\underset{\\theta}{\\text{argmax}} \\log p(x|\\theta) \\\\\n",
    "   &= \\underset{\\theta}{\\text{argmax}}  \\log  \\sum_{z} \\underbrace{p(z|\\theta) p(x|z,\\theta)}_{\\text{complete data likelihood}} \n",
    "\\end{align}\n",
    "\n",
    "The expression on the right is hard to evaluate as the summation over latent variables is almost always intractable. For this, classical EM algorithm attempts to maximize a lower bound for the likelihood. Now, let's first write the log-likelihood:\n",
    "\n",
    "\\begin{align}\n",
    "    \\log p(x|\\theta) &= \\log \\sum_{z} p(x,z|\\theta) \\\\\n",
    "                     &= \\log \\sum_{z} p(x,z|\\theta) \\frac{q(z)}{q(z)} \\\\  \n",
    "                     &= \\log E \\left[ \\frac{ p(x,z|\\theta)}{q(z)} \\right]_{q(z)} \\\\ \n",
    "                     &\\geq \\underbrace{E \\left[ \\log  \\frac{p(x,z|\\theta)}{q(z)} \\right]_{q(z)}}_{\\text{free energy term}} \\\\\n",
    "                     &= E \\left[ \\log p(z|\\theta) + \\log p(x|z,\\theta) - \\log q(z) \\right]_{q(z)} \\\\  \n",
    "                  \\mathcal{L}(\\theta, q(z)) &= \\underbrace{E \\left[ \\log p(z|\\theta) \\right]_{q(z)} + E \\left[ \\log p(x|z,\\theta) \\right]_{q(z)}}_{\\text{energy}} - \\underbrace{E \\left[ \\log q(z) \\right]_{q(z)}}_{\\text{entropy}} \\\\  \n",
    "\\end{align}\n",
    "\n",
    "Here, $q(z)$ is an arbitrary distribution on $z$. Also note that we used the Jensen inequality, which states that $f(E[x]) \\geq E[f(x)]$ when $f$ is a concave function (such as $\\log$), in the fourth line. In each iteration of the algorithm, $q(z)$ and $\\theta$ is updated to maximize current value of $\\mathcal{L}(\\theta, q(z))$. The algorithm terminates when the likelihood converges.\n",
    "\n",
    "#### E-Step\n",
    "In the classical EM, recipe for the E-step is ready. At $t$'th step,\n",
    "\\begin{equation}\n",
    "    q(z)^{(t)} = p(z|x,\\theta^{(t)})\n",
    "\\end{equation}\n",
    "\n",
    "This choice makes the bound tight:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}(p(z|x,\\theta), \\theta) &= E \\left[ \\log p(z|\\theta) + \\log p(x|z,\\theta) - \\log p(z|x,\\theta) \\right]_{p(z|x,\\theta)} \\\\\n",
    "    &= \\sum_{z} p(z|x,\\theta) \\log \\frac{p(x,z|\\theta)}{p(z|x,\\theta)} \\\\\n",
    "    &= \\sum_{z} p(z|x,\\theta) \\log p(x|\\theta) \\\\\n",
    "    &= \\log p(x|\\theta) \\sum_{z} p(z|x,\\theta)  \\\\\n",
    "    &= \\log p(x|\\theta)\n",
    "\\end{align}\n",
    "\n",
    "From now on, we will replace $\\mathcal{L}(\\theta, q(z))$ with $\\mathcal{L}(\\theta, \\theta^{(t)})$ as $q(z)$ is a distribution conditioned to $\\theta^{(t)}$.\n",
    "\n",
    "#### M-Step\n",
    "The M-Step updates the model parameter $\\theta$ that maximizes the expected complete data log likelihood.\n",
    "\\begin{equation}\n",
    "    \\theta^{(t+1)} = \\underset{\\theta}{\\text{argmax}} \\mathcal{L}(\\theta^{(t)}, \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "#### Relation to KL Divergence\n",
    "When we re-write the free energy term,\n",
    "\\begin{align}\n",
    "    \\mathcal{L}(\\theta, q(z)) &= E \\left[ \\log  \\frac{p(x,z|\\theta)}{q(z)} \\right]_{q(z)} \\\\\n",
    "    &= E \\left[ \\log  \\frac{p(x|\\theta)p(z|x,\\theta)}{q(z)} \\right]_{q(z)} \\\\\n",
    "    &= E \\left[ \\log p(x|\\theta) \\right]_{q(z)} + E\\left[ \\log \\frac{p(z|x,\\theta)}{q(z)} \\right]_{q(z)} \\\\\n",
    "    &= \\mathcal{L}(\\theta) - \\mathbf{KL}[q(x)||p(z|x,\\theta)]\n",
    "\\end{align}\n",
    "\n",
    "Thus, maximizing the lower bound corresponds to minimizing KL divergence between the variational distribution and the posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
